import streamlit as st
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains import RetrievalQA
from sentence_transformers import SentenceTransformer, util
import matplotlib.pyplot as plt
import numpy as np
# ========== UI Customization ==========
primary_color = "#1E90FF"
secondary_color = "#FF6347"
background_color = "#F5F5F5"
text_color = "#4561e9"

st.markdown(f"""
    <style>
    .stApp {{
        background-color: {background_color};
        color: {text_color};
    }}
    .stButton>button {{
        background-color: {primary_color};
        color: white;
        border-radius: 5px;
        border: none;
        padding: 10px 20px;
        font-size: 16px;
    }}
    .stFileUploader>div>div>div>button {{
        background-color: {secondary_color};
        color: white;
        border-radius: 5px;
        border: none;
        padding: 10px 20px;
        font-size: 16px;
    }}
    </style>
""", unsafe_allow_html=True)

st.title("üìÑ CV & JD")

# File Upload
cv_file = st.file_uploader("Upload CV (PDF)", type="pdf", key="cv")
jd_file = st.file_uploader("Upload Job Description (PDF)", type="pdf", key="jd")

# Function Definitions
@st.cache_resource
def build_qa_chain(uploaded_file):
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.getvalue())
    loader = PDFPlumberLoader("temp.pdf")
    docs = loader.load()

    embedder = HuggingFaceEmbeddings()
    splitter = SemanticChunker(embedder)
    chunks = splitter.split_documents(docs)

    vector = FAISS.from_documents(chunks, embedder)
    retriever = vector.as_retriever(search_type="similarity", search_kwargs={"k": 3})

    prompt_template = PromptTemplate.from_template("""
    B·∫°n ƒëang ƒë√≥ng vai tr√≤ l√† m·ªôt chuy√™n gia tuy·ªÉn d·ª•ng c√≥ kinh nghi·ªám trong vi·ªác ƒë√°nh gi√° m·ª©c ƒë·ªô ph√π h·ª£p gi·ªØa h·ªì s∆° ·ª©ng vi√™n (CV) v√† b·∫£n m√¥ t·∫£ c√¥ng vi·ªác (JD). 
    D·ª±a tr√™n c√°c th√¥ng tin trong ph·∫ßn context d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch trung th·ª±c v√† kh√°ch quan.

    - N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y tr·∫£ l·ªùi "T√¥i kh√¥ng ch·∫Øc d·ª±a tr√™n th√¥ng tin hi·ªán t·∫°i".
    - Kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng c√≥ trong context.

    Context: {context}
    Question: {question}
    Tr·∫£ l·ªùi chi ti·∫øt v√† ch√≠nh x√°c:
    """)

    llm = Ollama(model="deepseek-coder-v2:16b", temperature=0.1)
    llm_chain = LLMChain(llm=llm, prompt=prompt_template)
    combine_docs_chain = StuffDocumentsChain(
        llm_chain=llm_chain,
        document_variable_name="context",
        document_prompt=PromptTemplate(
            input_variables=["page_content", "source"],
            template="Context:\ncontent:{page_content}\nsource:{source}"
        )
    )
    qa_chain = RetrievalQA(combine_documents_chain=combine_docs_chain, retriever=retriever)
    return qa_chain

def extract_info(qa_chain, question):
    try:
        return qa_chain.run(question).strip()
    except:
        return ""

similarity_model = SentenceTransformer("all-MiniLM-L6-v2")
def calculate_similarity(text1, text2):
    if not text1 or not text2:
        return 0.0
    emb1 = similarity_model.encode(text1, convert_to_tensor=True)
    emb2 = similarity_model.encode(text2, convert_to_tensor=True)
    return round(util.pytorch_cos_sim(emb1, emb2).item(), 2)

def draw_radar(scores_dict):
    labels = list(scores_dict.keys())
    values = list(scores_dict.values())

    angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()
    values += values[:1]
    angles += angles[:1]

    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
    ax.plot(angles, values, 'o-', linewidth=2)
    ax.fill(angles, values, alpha=0.25)
    ax.set_thetagrids(np.degrees(angles[:-1]), labels)
    ax.set_ylim(0, 100)
    ax.set_title("Bi·ªÉu ƒë·ªì ƒë√°nh gi√° m·ª©c ƒë·ªô ph√π h·ª£p", fontsize=16)
    st.pyplot(fig)

# ========= Process =========
if cv_file and jd_file:
    with st.spinner("üîç ƒêang ph√¢n t√≠ch CV v√† JD..."):
        cv_qa = build_qa_chain(cv_file)
        jd_qa = build_qa_chain(jd_file)

        fields = {
            "K·ªπ nƒÉng": "Li·ªát k√™ t·∫•t c·∫£ k·ªπ nƒÉng c·ªßa ·ª©ng vi√™n.",
            "H·ªçc v·∫•n": "Tr√¨nh ƒë·ªô h·ªçc v·∫•n c·ªßa ·ª©ng vi√™n l√† g√¨?",
            "Kinh nghi·ªám": "·ª®ng vi√™n c√≥ kinh nghi·ªám g√¨?",
            "B·∫±ng c·∫•p": "·ª®ng vi√™n c√≥ nh·ªØng b·∫±ng c·∫•p n√†o?",
            "V·ªã tr√≠ mong mu·ªën": "V·ªã tr√≠ ·ª©ng vi√™n mong mu·ªën l√† g√¨?"
        }

        total_score = 0
        radar_scores = {}
        st.subheader("K·∫øt qu·∫£ ƒë√°nh gi√° t·ª´ng m·ª•c")

        for name, question in fields.items():
            cv_answer = extract_info(cv_qa, question)
            jd_question = f"Job description y√™u c·∫ßu g√¨ v·ªÅ: {name.lower()}?"
            jd_answer = extract_info(jd_qa, jd_question)
            score = calculate_similarity(cv_answer, jd_answer)
            total_score += score
            radar_scores[name] = score * 100

            with st.expander(f"üîç {name} (T∆∞∆°ng ƒë·ªìng: {score * 100:.0f}%)"):
                st.markdown(f"**T·ª´ CV:** {cv_answer or 'Kh√¥ng t√¨m th·∫•y'}")
                st.markdown(f"**T·ª´ JD:** {jd_answer or 'Kh√¥ng t√¨m th·∫•y'}")

        avg_score = total_score / len(fields)
        st.success(f"M·ª©c ƒë·ªô ph√π h·ª£p t·ªïng th·ªÉ: **{avg_score * 100:.0f}%**")
        draw_radar(radar_scores)
else:
    st.warning("Vui l√≤ng upload **c·∫£ CV** v√† **JD** ƒë·ªÉ h·ªá th·ªëng ph√¢n t√≠ch.")
